{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carvana exploration notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, BatchNormalization, Conv2DTranspose\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the file name IDs and split into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "dataset_dir = '/home/ubuntu/carvana/input/data/'\n",
    "image_dir = dataset_dir + 'images'\n",
    "mask_dir = dataset_dir + 'masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 128\n",
    "img_height = 128\n",
    "batch_size = 16\n",
    "epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4071 images belonging to 1 classes.\n",
      "Found 4071 images belonging to 1 classes.\n",
      "Found 1017 images belonging to 1 classes.\n",
      "Found 1017 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define data augmentations for training set\n",
    "data_gen_args = dict(rescale=1./255,\n",
    "                     shear_range=0.1,\n",
    "                     rotation_range=4,\n",
    "                     zoom_range=0.1,\n",
    "                     horizontal_flip=True,\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     validation_split=0.2) # 20% validation set\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Create generator for training images\n",
    "train_image_generator = image_datagen.flow_from_directory(\n",
    "    image_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    subset='training')\n",
    "\n",
    "# Create generator for training masks\n",
    "train_mask_generator = mask_datagen.flow_from_directory(\n",
    "    mask_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    subset='training')\n",
    "\n",
    "# Create generator for validation images\n",
    "val_image_generator = image_datagen.flow_from_directory(\n",
    "    image_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    subset='validation')\n",
    "\n",
    "# Create generator for validation masks\n",
    "val_mask_generator = mask_datagen.flow_from_directory(\n",
    "    mask_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    subset='validation')\n",
    "\n",
    "num_samples_train = train_image_generator.n\n",
    "num_samples_val = val_image_generator.n\n",
    "\n",
    "# Combine generators into single training and validation generators for model training\n",
    "train_generator = zip(train_image_generator, train_mask_generator)\n",
    "validation_generator = zip(val_image_generator, val_mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(true, pred):\n",
    "    smooth = 1.\n",
    "    true_flat = K.flatten(true)\n",
    "    pred_flat = K.flatten(pred)\n",
    "    intersection = K.sum(true_flat * pred_flat)\n",
    "    score = (2. * intersection + smooth) / (K.sum(true_flat) + K.sum(pred_flat) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(true, pred):\n",
    "    loss = 1 - dice_coeff(true, pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_base(input_shape=(128,128,3),\n",
    "              num_classes=3, \n",
    "              first_filters=64):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    down1 = Conv2D(first_filters, (3,3), activation='relu', padding='same')(inputs)\n",
    "    down1 = BatchNormalization()(down1)\n",
    "    down1 = Conv2D(first_filters, (3,3), activation='relu', padding='same')(down1)\n",
    "    down1 = BatchNormalization()(down1)\n",
    "    down1_pool = MaxPooling2D((2,2), strides=(2,2))(down1)\n",
    "    print(down1.shape)\n",
    "    \n",
    "    down2 = Conv2D(first_filters*2, (3,3), activation='relu', padding='same')(down1_pool)\n",
    "    down2 = BatchNormalization()(down2)\n",
    "    down2 = Conv2D(first_filters*2, (3,3), activation='relu', padding='same')(down2)\n",
    "    down2 = BatchNormalization()(down2)\n",
    "    down2_pool = MaxPooling2D((2,2), strides=(2,2))(down2)\n",
    "    print(down2.shape)\n",
    "    \n",
    "    down3 = Conv2D(first_filters*4, (3,3), activation='relu', padding='same')(down2_pool)\n",
    "    down3 = BatchNormalization()(down3)\n",
    "    down3 = Conv2D(first_filters*4, (3,3), activation='relu', padding='same')(down3)\n",
    "    down3 = BatchNormalization()(down3)\n",
    "    down3_pool = MaxPooling2D((2,2), strides=(2,2))(down3)\n",
    "    print(down3.shape)\n",
    "    \n",
    "    center = Conv2D(first_filters*8, (3,3), activation='relu', padding='same')(down3_pool)\n",
    "    center = BatchNormalization()(center)\n",
    "    center = Conv2D(first_filters*8, (3,3), activation='relu', padding='same')(center)\n",
    "    center = BatchNormalization()(center)\n",
    "    print(center.shape)\n",
    "    \n",
    "    # up3 = UpSampling2D((2,2))(center)\n",
    "    up3 = Conv2DTranspose(first_filters*4, (2,2), activation='relu', strides=(2,2), padding='same')(center)\n",
    "    up3 = concatenate([up3, down3], axis=3)\n",
    "    up3 = Conv2D(first_filters*4, (3,3), activation='relu', padding='same')(up3)\n",
    "    up3 = BatchNormalization()(up3)\n",
    "    up3 = Conv2D(first_filters*4, (3,3), activation='relu', padding='same')(up3)\n",
    "    up3 = BatchNormalization()(up3)\n",
    "    \n",
    "    # up2 = UpSampling2D((2,2))(up3)\n",
    "    up2 = Conv2DTranspose(first_filters*2, (2,2), activation='relu', strides=(2,2), padding='same')(up3)\n",
    "    up2 = concatenate([up2, down2], axis=3)\n",
    "    up2 = Conv2D(first_filters*2, (3,3), activation='relu', padding='same')(up2)\n",
    "    up2 = BatchNormalization()(up2)\n",
    "    up2 = Conv2D(first_filters*2, (3,3), activation='relu', padding='same')(up2)\n",
    "    up2 = BatchNormalization()(up2)\n",
    "    \n",
    "    # up1 = UpSampling2D((2,2))(up2)\n",
    "    up1 = Conv2DTranspose(first_filters, (2,2), activation='relu', strides=(2,2), padding='same')(up2)\n",
    "    up1 = concatenate([up1, down1], axis=3)\n",
    "    up1 = Conv2D(first_filters, (3,3), activation='relu', padding='same')(up1)\n",
    "    up1 = BatchNormalization()(up1)\n",
    "    up1 = Conv2D(first_filters, (3,3), activation='relu', padding='same')(up1)\n",
    "    up1 = BatchNormalization()(up1)\n",
    "    \n",
    "    classify = Conv2D(num_classes, (1, 1), activation='sigmoid')(up1)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=classify)\n",
    "    \n",
    "    model.compile(optimizer=RMSprop(lr=0.0001), loss=dice_loss, metrics=[dice_coeff])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='weights/best_weights.h5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True),\n",
    "             TensorBoard(log_dir='logs')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 128, 64)\n",
      "(?, 64, 64, 128)\n",
      "(?, 32, 32, 256)\n",
      "(?, 16, 16, 512)\n",
      "Epoch 1/80\n",
      " - 189s - loss: 0.1552 - dice_coeff: 0.8448 - val_loss: 0.1594 - val_dice_coeff: 0.8406\n",
      "Epoch 2/80\n"
     ]
    }
   ],
   "source": [
    "model = unet_base()\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=np.ceil(num_samples_train/batch_size),\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=np.ceil(num_samples_val/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
